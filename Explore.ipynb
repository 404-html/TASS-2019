{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load needed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ftfy\n",
    "\n",
    "nlp = spacy.load('es')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet = namedtuple('Tweet', ['tweetid', 'content', 'polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from given file and return it as a dataframe.\"\"\"\n",
    "    tweets: List = []\n",
    "    with open(file, 'r') as f:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            tweets.append(tweet(child[0].text, child[2].text, child[5][0][0].text))\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "def read_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from given folder, combines the training and dev set\n",
    "    and return them combined as a dataframe.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    files = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    for file in files:\n",
    "        if 'xml' in file:\n",
    "            dataframes.append(read_data(folder + file))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mention(tweet: str) -> str:\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', tweet) \n",
    "\n",
    "def lower_case(tweet: str) -> str:\n",
    "    \"\"\"Turn a tweet to lower case.\"\"\"\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_question_mark(tweet: str) -> str:\n",
    "    \"\"\"Remove spanish question mark from a tweet.\"\"\"\n",
    "    return tweet.replace('¿', '')\n",
    "\n",
    "def remove_punctuation(tweet: str) -> str:\n",
    "    \"\"\"Remove punctuation from a tweet.\"\"\"\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_whitespace(tweet: str) -> str:\n",
    "    return tweet.strip()\n",
    "\n",
    "def check_int(s):\n",
    "    if s[0] in ('-', '+'):\n",
    "        return s[1:].isdigit()\n",
    "    return s.isdigit()\n",
    "\n",
    "def remove_numbers(tweet: str) -> str:\n",
    "    \"\"\"Remove numbers from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if not check_int(word)])\n",
    "\n",
    "def fix_encoding(tweet: str) -> str:\n",
    "    return ftfy.fix_encoding(tweet)\n",
    "\n",
    "def fix_repeated_letters(tweet: str) -> str:\n",
    "    \"\"\"Replace repeated characters (3 repetitions or more) with only two characters.\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_mention,\n",
    "                 lower_case,\n",
    "                 remove_question_mark,\n",
    "                 remove_punctuation,\n",
    "                 remove_whitespace,\n",
    "                 remove_numbers,\n",
    "                 fix_encoding,\n",
    "                 fix_repeated_letters\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling\n",
    "Preprocessing that should be done after spell correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='es')\n",
    "\n",
    "def fix_spelling(tweet: str) -> str:\n",
    "    \"\"\"Fix spelling error in tweets.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(tokenized)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] in misspelled:\n",
    "            contents = urllib.request.urlopen(f\"http://api.urbandictionary.com/v0/define?term={tokenized[i]}\").read()\n",
    "            contents = json.loads(contents)['list']\n",
    "            \n",
    "            # Check if the word is spelling error or is in urban dictionary\n",
    "            if contents and contents[0]['word'].lower().strip() == tokenized[i]:\n",
    "                continue\n",
    "            tokenized[i] = spell.correction(tokenized[i])\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "def remove_stopwords(tweet: str) -> str:\n",
    "    \"\"\"Remove stopwords from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if word not in stopWords])\n",
    "\n",
    "def stem_tweet(tweet: str) -> str:\n",
    "    tweet = nlp(tweet)\n",
    "    return ' '.join([token.lemma_ for token in tweet])\n",
    "\n",
    "\n",
    "def clean_tweet2(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_stopwords,\n",
    "                 stem_tweet\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1167/1167 [00:00<00:00, 3662.61it/s]\n",
      "100%|██████████| 1167/1167 [00:11<00:00, 101.56it/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_folder(\"data/cr/\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet)\n",
    "# df['content'] = df['content'].progress_apply(fix_spelling)\n",
    "df['content'] = df['content'].progress_apply(clean_tweet2)\n",
    "df.to_csv(\"data/cr/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1167 entries, 0 to 389\n",
      "Data columns (total 3 columns):\n",
      "tweetid     1167 non-null object\n",
      "content     1167 non-null object\n",
      "polarity    1167 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 36.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tass_2019",
   "language": "python",
   "name": "tass_2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
