{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load needed things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import ftfy\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "nlp = spacy.load('es')\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet = namedtuple('Tweet', ['tweetid', 'content', 'polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from given file and return it as a dataframe.\"\"\"\n",
    "    tweets: List = []\n",
    "    with open(file, 'r') as f:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            tweets.append(tweet(child[0].text, child[2].text, child[5][0][0].text))\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "def read_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from given folder, combines the training and dev set\n",
    "    and return them combined as a dataframe.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    files = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    for file in files:\n",
    "        if 'xml' in file:\n",
    "            dataframes.append(read_data(folder + file))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### String manipulation\n",
    "\n",
    "Don't need to care for emoticons, because there are less than 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_mention(tweet: str) -> str:\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', tweet) \n",
    "\n",
    "def lower_case(tweet: str) -> str:\n",
    "    \"\"\"Turn a tweet to lower case.\"\"\"\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_question_mark(tweet: str) -> str:\n",
    "    \"\"\"Remove spanish question mark from a tweet.\"\"\"\n",
    "    return tweet.replace('¿', '')\n",
    "\n",
    "def remove_punctuation(tweet: str) -> str:\n",
    "    \"\"\"Remove punctuation from a tweet.\"\"\"\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_whitespace(tweet: str) -> str:\n",
    "    return tweet.strip()\n",
    "\n",
    "def check_int(s):\n",
    "    if s[0] in ('-', '+'):\n",
    "        return s[1:].isdigit()\n",
    "    return s.isdigit()\n",
    "\n",
    "def remove_numbers(tweet: str) -> str:\n",
    "    \"\"\"Remove numbers from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if not check_int(word)])\n",
    "\n",
    "def fix_encoding(tweet: str) -> str:\n",
    "    return ftfy.fix_encoding(tweet)\n",
    "\n",
    "def fix_repeated_letters(tweet: str) -> str:\n",
    "    \"\"\"Replace repeated characters (3 repetitions or more) with only two characters.\"\"\"\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', tweet)\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_mention,\n",
    "                 lower_case,\n",
    "                 remove_question_mark,\n",
    "                 remove_punctuation,\n",
    "                 remove_whitespace,\n",
    "                 remove_numbers,\n",
    "                 fix_encoding,\n",
    "                 fix_repeated_letters\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Spelling\n",
    "Preprocessing that should be done after spell correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='es')\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "def fix_spelling(tweet: str) -> str:\n",
    "    \"\"\"Fix spelling error in tweets.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(tokenized)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] in misspelled:\n",
    "            if is_ascii(tokenized[i]):\n",
    "                contents = urllib.request.urlopen(f\"http://api.urbandictionary.com/v0/define?term={tokenized[i]}\").read()\n",
    "                contents = json.loads(contents)['list']\n",
    "            \n",
    "                # Check if the word is spelling error or is in urban dictionary\n",
    "                if contents and contents[0]['word'].lower().strip() == tokenized[i]:\n",
    "                    continue\n",
    "            tokenized[i] = spell.correction(tokenized[i])\n",
    "    return ' '.join(tokenized)\n",
    "\n",
    "def remove_stopwords(tweet: str) -> str:\n",
    "    \"\"\"Remove stopwords from tweet.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokenized if word not in stopWords])\n",
    "\n",
    "def stem_tweet(tweet: str) -> str:\n",
    "    tweet = nlp(tweet)\n",
    "    return ' '.join([token.lemma_ for token in tweet])\n",
    "\n",
    "\n",
    "def clean_tweet2(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_stopwords,\n",
    "                 stem_tweet\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Clean and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1167/1167 [00:00<00:00, 2593.14it/s]\n",
      "100%|██████████| 1167/1167 [19:43<00:00,  1.01s/it] \n",
      "100%|██████████| 1167/1167 [00:16<00:00, 71.88it/s]\n"
     ]
    }
   ],
   "source": [
    "df = read_folder(\"data/cr/\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet)\n",
    "df['content'] = df['content'].progress_apply(fix_spelling)\n",
    "df['content'] = df['content'].progress_apply(clean_tweet2)\n",
    "df.to_csv(\"data/cr/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cr/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweetid</th>\n",
       "      <th>content</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>768225400254111744</td>\n",
       "      <td>totalmente puntual</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>770077064833671168</td>\n",
       "      <td>hola sandrita habia desear feliz dia madre tar...</td>\n",
       "      <td>P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>771207534342320128</td>\n",
       "      <td>si andar hacer mejor quedar calladita jaja asi...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>771900763987513345</td>\n",
       "      <td>pereza querer chocar banano</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>772550560998301697</td>\n",
       "      <td>bueno mayor cuánto campar tú sos cartaguito ca...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             tweetid  \\\n",
       "0           0  768225400254111744   \n",
       "1           1  770077064833671168   \n",
       "2           2  771207534342320128   \n",
       "3           3  771900763987513345   \n",
       "4           4  772550560998301697   \n",
       "\n",
       "                                             content polarity  \n",
       "0                                 totalmente puntual     NONE  \n",
       "1  hola sandrita habia desear feliz dia madre tar...        P  \n",
       "2  si andar hacer mejor quedar calladita jaja asi...        N  \n",
       "3                        pereza querer chocar banano        N  \n",
       "4  bueno mayor cuánto campar tú sos cartaguito ca...        N  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1167 entries, 0 to 1166\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    1167 non-null int64\n",
      "tweetid       1167 non-null int64\n",
      "content       1167 non-null object\n",
      "polarity      1167 non-null object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 36.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f96083b21d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "polarity_counts = Counter(df.polarity.values)\n",
    "df = pd.DataFrame.from_dict(polarity_counts, orient='index')\n",
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn tweets into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist([word for tweet in df.content for word in nltk.word_tokenize(tweet)])\n",
    "word_features = list(all_words)[:2000] # [_document-classify-all-words]\n",
    "\n",
    "def document_features(document): # [_document-classify-extractor]\n",
    "    document_words = set(nltk.word_tokenize(document)) # [_document-classify-set]\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "featuresets = [(document_features(d), c) for (d,c) in zip(df.content, df.polarity)]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(lindar) = True                P : N      =     13.9 : 1.0\n",
      "        contains(triste) = True                N : P      =     12.6 : 1.0\n",
      "         contains(final) = True              NEU : P      =     10.1 : 1.0\n",
      "          contains(gran) = True                P : N      =      9.4 : 1.0\n",
      "       contains(cambiar) = True              NEU : P      =      8.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tass_2019",
   "language": "python",
   "name": "tass_2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
