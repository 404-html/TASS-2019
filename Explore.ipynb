{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, List\n",
    "import unidecode\n",
    "from spellchecker import SpellChecker\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define named tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tweet = namedtuple('Tweet', ['tweetid', 'content', 'polarity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Define stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopWords = set(nltk.corpus.stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data(file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read data from given file and return it as a dataframe.\"\"\"\n",
    "    tweets: List = []\n",
    "    with open(file, 'r') as f:\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        for child in root:\n",
    "            tweets.append(tweet(child[0].text, child[2].text, child[5][0][0].text))\n",
    "    return pd.DataFrame(tweets)\n",
    "\n",
    "def read_folder(folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from given folder, combines the training and dev set\n",
    "    and return them combined as a dataframe.\n",
    "    \"\"\"\n",
    "    dataframes = []\n",
    "    files = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
    "    for file in files:\n",
    "        if 'xml' in file:\n",
    "            dataframes.append(read_data(folder + file))\n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preprocessing tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def remove_mention(tweet: str) -> str:\n",
    "    return re.sub(r'@[A-Za-z0-9]+', '', tweet) \n",
    "\n",
    "def lower_case(tweet: str) -> str:\n",
    "    \"\"\"Turn a tweet to lower case.\"\"\"\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_question_mark(tweet: str) -> str:\n",
    "    \"\"\"Remove spanish question mark from a tweet.\"\"\"\n",
    "    return tweet.replace('¿', '')\n",
    "\n",
    "def remove_punctuation(tweet: str) -> str:\n",
    "    \"\"\"Remove punctuation from a tweet.\"\"\"\n",
    "    return tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_accents(tweet: str) -> str:\n",
    "    \"\"\"Remove accents from a tweet.\"\"\"\n",
    "    return unidecode.unidecode(tweet)\n",
    "\n",
    "def remove_whitespace(tweet: str) -> str:\n",
    "    return tweet.strip()\n",
    "\n",
    "def clean_tweet(tweet: str) -> str:\n",
    "    \"\"\"Run a tweet through cleaning pipeline.\"\"\"\n",
    "    # List of function\n",
    "    functions: List[Callable] = [\n",
    "                 remove_mention,\n",
    "                 lower_case,\n",
    "                 remove_question_mark,\n",
    "                 remove_punctuation,\n",
    "                 remove_accents,\n",
    "                 remove_whitespace\n",
    "                 ]\n",
    "    for f in functions:\n",
    "        tweet = f(tweet)\n",
    "        \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker(language='es')\n",
    "\n",
    "def fix_spelling(tweet: str) -> str:\n",
    "    \"\"\"Fix spelling error in tweets.\"\"\"\n",
    "    tokenized = nltk.word_tokenize(tweet)\n",
    "    misspelled = spell.unknown(tokenized)\n",
    "    for i in range(len(tokenized)):\n",
    "        if tokenized[i] in misspelled:\n",
    "            contents = urllib.request.urlopen(f\"http://api.urbandictionary.com/v0/define?term={tokenized[i]}\").read()\n",
    "            contents = json.loads(contents)['list']\n",
    "            \n",
    "            # Check if the word is spelling error or is in urban dictionary\n",
    "            if contents and contents[0]['word'].lower().strip() == tokenized[i]:\n",
    "                continue\n",
    "            tokenized[i] = spell.correction(tokenized[i])\n",
    "    return ' '.join(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tweet: str) -> str:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1167/1167 [00:00<00:00, 25261.41it/s]\n",
      "100%|██████████| 1167/1167 [16:50<00:00,  1.15it/s] \n"
     ]
    }
   ],
   "source": [
    "df = read_folder(\"data/cr/\")\n",
    "df['content'] = df['content'].progress_apply(clean_tweet)\n",
    "df['content'] = df['content'].progress_apply(fix_spelling)\n",
    "# df.to_csv(data/cr/cleaned.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tass_2019",
   "language": "python",
   "name": "tass_2019"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
